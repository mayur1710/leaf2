# -*- coding: utf-8 -*-
"""h5.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1frZXvs0X6R8Z1JwAexYEu69I1IQNFlwt
"""

import requests
import os
import os
import numpy as np
import tensorflow
from tensorflow.keras.applications import MobileNetV2,Model,Dense,GlobalAveragePooling2D,ImageDAtaGenerator,Adam


# URL of the dataset ZIP file
url = "https://github.com/spMohanty/PlantVillage-Dataset/archive/master.zip"

# Send a GET request to download the dataset ZIP file
response = requests.get(url)

# Save the downloaded ZIP file to disk
zip_file_path = "/content/PlantVillage-Dataset.zip"
with open(zip_file_path, "wb") as f:
    f.write(response.content)

print("Dataset downloaded successfully.")
print("Location of dataset:", zip_file_path)

print("Location of dataset:", zip_file_path)

import os
import zipfile
import shutil
from sklearn.model_selection import train_test_split

# Define paths
zip_file_path = "/content/PlantVillage-Dataset.zip"
output_dir = "/content/preprocessed_dataset"

# Create output directory
os.makedirs(output_dir, exist_ok=True)

# Extract the ZIP file
with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    # Extract all contents to the output directory
    zip_ref.extractall(output_dir)

# Define dataset directory
dataset_dir = os.path.join(output_dir, "PlantVillage-Dataset-master", "raw", "color")

# Create directories for train, val, and test sets
train_dir = os.path.join(output_dir, "train")
val_dir = os.path.join(output_dir, "val")
test_dir = os.path.join(output_dir, "test")
os.makedirs(train_dir, exist_ok=True)
os.makedirs(val_dir, exist_ok=True)
os.makedirs(test_dir, exist_ok=True)

# List subdirectories (plant diseases)
subdirs = [subdir for subdir in os.listdir(dataset_dir) if os.path.isdir(os.path.join(dataset_dir, subdir))]

# Split dataset into train, validation, and test sets
for subdir in subdirs:
    subdir_path = os.path.join(dataset_dir, subdir)
    images = [image for image in os.listdir(subdir_path) if image.endswith(".jpg")]

    # Ensure there are enough images for splitting
    if len(images) < 2:
        print(f"Not enough images for splitting in {subdir}, skipping.")
        continue

    train_images, test_images = train_test_split(images, test_size=0.2, random_state=42)
    val_images, test_images = train_test_split(test_images, test_size=0.5, random_state=42)

    # Copy images to train, val, and test directories
    for image in train_images:
        src = os.path.join(subdir_path, image)
        dest = os.path.join(train_dir, subdir)
        os.makedirs(dest, exist_ok=True)
        shutil.copy(src, dest)

    for image in val_images:
        src = os.path.join(subdir_path, image)
        dest = os.path.join(val_dir, subdir)
        os.makedirs(dest, exist_ok=True)
        shutil.copy(src, dest)

    for image in test_images:
        src = os.path.join(subdir_path, image)
        dest = os.path.join(test_dir, subdir)
        os.makedirs(dest, exist_ok=True)
        shutil.copy(src, dest)

print("Dataset processed successfully.")



# Define paths
train_dir = "/content/preprocessed_dataset/train"
val_dir = "/content/preprocessed_dataset/val"
test_dir = "/content/preprocessed_dataset/test"

# Define image dimensions
img_height, img_width = 224, 224
num_classes = len(os.listdir(train_dir))

# Create data generators
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

val_datagen = ImageDataGenerator(rescale=1./255)
test_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(img_height, img_width),
    batch_size=32,
    class_mode='categorical'
)

val_generator = val_datagen.flow_from_directory(
    val_dir,
    target_size=(img_height, img_width),
    batch_size=32,
    class_mode='categorical'
)

test_generator = test_datagen.flow_from_directory(
    test_dir,
    target_size=(img_height, img_width),
    batch_size=32,
    class_mode='categorical'
)

# Load pre-trained MobileNetV2 model without top layer
base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))

# Add custom top layers for our classification task
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(1024, activation='relu')(x)
predictions = Dense(num_classes, activation='softmax')(x)

# Combine base model with top layers
model = Model(inputs=base_model.input, outputs=predictions)

# Freeze base layers during training
for layer in base_model.layers:
    layer.trainable = False

# Compile the model
model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(train_generator, epochs=3, validation_data=val_generator)

# Evaluate the model on the test set
loss, accuracy = model.evaluate(test_generator)
print("Test Accuracy:", accuracy)

# Save the model for deployment
model.save("plant_disease_classifier_mobilenetv2.h5")

import os
import zipfile

# Define the path to the downloaded ZIP file
zip_file_path = '/content/PlantVillage-Dataset.zip'

# Define the directory where you want to extract the dataset
extracted_dir = '/content/PlantVillage-Dataset'

# Extract the dataset
with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    zip_ref.extractall(extracted_dir)

print("Dataset extracted successfully.")

import os
print(os.listdir('/content/PlantVillage-Dataset'))

dataset_dir = "/content/PlantVillage-Dataset-master/raw/color"

from tensorflow.keras.applications import ResNet50

# Load the ResNet50 model pre-trained on ImageNet without the top layer
base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Print summary of the base model
base_model.summary()

from tensorflow.keras.models import Model
from tensorflow.keras.layers import GlobalAveragePooling2D, Dense

# Add new layers on top of the base model
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(1024, activation='relu')(x)
predictions = Dense(num_classes, activation='softmax')(x)

# Combine base model with new top layers
model = Model(inputs=base_model.input, outputs=predictions)

# Print summary of the model
model.summary()

from tensorflow.keras.optimizers import Adam

# Compile the model
model.compile(optimizer=Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(train_generator, epochs=1, validation_data=val_generator)

# Evaluate the model on the test set
test_loss, test_accuracy = model.evaluate(test_generator)
print(f'Test loss: {test_loss}')
print(f'Test accuracy: {test_accuracy}')
import matplotlib.pyplot as plt

# Plot training loss and validation loss
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.legend()
plt.show()

# Plot training accuracy and validation accuracy
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Training and Validation Accuracy')
plt.legend()
plt.show()

# Fine-tuning the model if necessary
# Adjust hyperparameters, architecture, or apply data augmentation

# Evaluate the model on the test set
test_loss, test_accuracy = model.evaluate(test_generator)
print(f'Test Loss: {test_loss}')
print(f'Test Accuracy: {test_accuracy}')

# Evaluate the model on the test set
test_loss, test_accuracy = model.evaluate(test_generator)
print(f'Test Loss: {test_loss}')
print(f'Test Accuracy: {test_accuracy}')

# Interpret test results
if test_accuracy > 0.7:
    print("The model's performance on the test set is satisfactory.")
    print("You can consider deploying the model for real-world use.")
else:
    print("The model's performance on the test set needs improvement.")
    print("You may need to further fine-tune the model or explore different architectures.")

# Further steps
# If the model's performance is satisfactory, proceed with deployment and documentation.
# If not, consider additional model improvement techniques such as hyperparameter tuning or data augmentation.
# Iterate on the model based on feedback and new requirements.